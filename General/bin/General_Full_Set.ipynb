{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script was modified from the \"General.ipynb\" script, which was created for the purpose of benchmarking of ML methods. This script is meant to train the same networks present in \"General.ipynb\" with the same training set, and classify the new data collected with belt.\n",
    "\n",
    "Created by Samuel Horovatin, February 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing Imports\n",
    "import pandas as pd\n",
    "import os\n",
    "from skimage.transform import resize, rotate, resize \n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "# Logger Imports\n",
    "import time\n",
    "import logging\n",
    "import sys\n",
    "from logging.handlers import TimedRotatingFileHandler\n",
    "\n",
    "# Model Specific Imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sets up all standardized logger stuff\n",
    "FORMATTER = logging.Formatter(\"%(asctime)s —  %(levelname)s — %(message)s\")\n",
    "LOG_FILE = \"General_Script_Full_Set.log\"\n",
    "\n",
    "def get_console_handler():\n",
    "    console_handler = logging.StreamHandler(sys.stdout)\n",
    "    console_handler.setFormatter(FORMATTER)\n",
    "    return console_handler\n",
    "\n",
    "def get_file_handler():\n",
    "    file_handler = TimedRotatingFileHandler(LOG_FILE, when='midnight')\n",
    "    file_handler.setFormatter(FORMATTER)\n",
    "    return file_handler\n",
    "\n",
    "def get_logger(logger_name):\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    \n",
    "    if (logger.hasHandlers()): # important as removes duplicate loggers (and thus duplicate log entries)\n",
    "        logger.handlers.clear()\n",
    "    logger.setLevel(logging.DEBUG) # better to have too much log than not enough\n",
    "    logger.addHandler(get_console_handler())\n",
    "    logger.addHandler(get_file_handler())\n",
    "    # with this pattern, it's rarely necessary to propagate the error up to parent\n",
    "    logger.propagate = False\n",
    "    return logger\n",
    "\n",
    "logger = get_logger(\"General_Script\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lays out all training data paths. \n",
    "# NOTE, the folder in which the img's are stored in dictates there health class. See comments for folder class\n",
    "\n",
    "Categories_Durum=['CFP-CK1','CFP-CK2'] # [Infected, Healthy]\n",
    "Categories_Bread=['CFP-CK3','CFP-CK4'] # [Infected, Healthy]\n",
    "Categories_Test_B179 = ['CFP-B179-A','CFP-B179-B'] # [Infected, Healthy]\n",
    "Categories_Test_B223 = ['CFP-B223-A','CFP-B223-B'] # [Infected, Healthy]\n",
    "\n",
    "Infected_Categories = ['CFP-CK1', 'CFP-CK3', 'CFP-B179-A', 'CFP-B223-A']\n",
    "\n",
    "All_Categories = [Categories_Durum, Categories_Bread, Categories_Test_B179, Categories_Test_B223]\n",
    "\n",
    "# Pickle Info Locations\n",
    "Pickle_location = '/birl2/users/sch923/Thesis/Data/'\n",
    "Eval_Data_Pickle_Name = 'Eval_Data.pkl'\n",
    "Models_Pickle_Name = 'Models_L1_LogR.pkl'\n",
    "Prediction_Pickle_Name = 'Predictions_L1_LogR.pkl'\n",
    "\n",
    "# Training Set Info Locations\n",
    "datadir='/birl2/users/sch923/Thesis/Data/Wheat/TestSamples' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-01 08:34:46,413 —  INFO — loading category: CFP-CK1\n",
      "2022-03-01 08:35:04,000 —  INFO — loaded category: CFP-CK1 successfully, found 43 images\n",
      "2022-03-01 08:35:04,002 —  INFO — loading category: CFP-CK2\n",
      "2022-03-01 08:35:12,544 —  INFO — loaded category: CFP-CK2 successfully, found 48 images\n",
      "2022-03-01 08:35:12,546 —  INFO — loading category: CFP-CK3\n",
      "2022-03-01 08:35:16,042 —  INFO — loaded category: CFP-CK3 successfully, found 24 images\n",
      "2022-03-01 08:35:16,043 —  INFO — loading category: CFP-CK4\n",
      "2022-03-01 08:35:23,805 —  INFO — loaded category: CFP-CK4 successfully, found 31 images\n",
      "2022-03-01 08:35:23,806 —  INFO — loading category: CFP-B179-A\n",
      "2022-03-01 08:35:30,951 —  INFO — loaded category: CFP-B179-A successfully, found 29 images\n",
      "2022-03-01 08:35:30,952 —  INFO — loading category: CFP-B179-B\n",
      "2022-03-01 08:35:41,858 —  INFO — loaded category: CFP-B179-B successfully, found 74 images\n",
      "2022-03-01 08:35:41,859 —  INFO — loading category: CFP-B223-A\n",
      "2022-03-01 08:35:43,928 —  INFO — loaded category: CFP-B223-A successfully, found 14 images\n",
      "2022-03-01 08:35:43,928 —  INFO — loading category: CFP-B223-B\n",
      "2022-03-01 08:35:56,713 —  INFO — loaded category: CFP-B223-B successfully, found 82 images\n",
      "2022-03-01 08:35:56,715 —  INFO — Number Of Healthy Kernel Images Found: 235\n",
      "2022-03-01 08:35:56,716 —  INFO — Number Of Infected Kernel Images Found: 110\n",
      "2022-03-01 08:35:56,716 —  INFO — Total Number Of Images Found: 345\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing step and loading of all training data\n",
    "\n",
    "img_arr, label_arr = [],[]\n",
    "total_imgs, total_infected, total_healthy = 0, 0, 0\n",
    "\n",
    "\n",
    "# creating species specific data sets\n",
    "for Categories in All_Categories:\n",
    "    img_arr.append(list())\n",
    "    label_arr.append(list())\n",
    "    \n",
    "    for i in Categories:\n",
    "        logger.info(f'loading category: {i}')    \n",
    "        path=os.path.join(datadir,i) \n",
    "        image_count = 0\n",
    "        \n",
    "        for img_path in os.listdir(path):  \n",
    "            img=imread(os.path.join(path,img_path))\n",
    "            img_resized=resize(img,(150,150,3))  \n",
    "            img_arr[len(img_arr)-1].append(img_resized) # Saves images\n",
    "            label_arr[len(img_arr)-1].append(Categories.index(i)) # Applies category based on folder\n",
    "            image_count += 1\n",
    "            \n",
    "        logger.info(f'loaded category: {i} successfully, found {image_count} images')\n",
    "\n",
    "        # Entire Dataset metric collection\n",
    "        total_imgs = total_imgs + image_count\n",
    "        if i in Infected_Categories:\n",
    "            total_infected = total_infected + image_count\n",
    "        else:\n",
    "            total_healthy = total_healthy + image_count\n",
    "\n",
    "logger.info(f'Number Of Healthy Kernel Images Found: {total_healthy}')            \n",
    "logger.info(f'Number Of Infected Kernel Images Found: {total_infected}')\n",
    "logger.info(f'Total Number Of Images Found: {total_imgs}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-01 08:35:56,921 —  INFO — Flattening images...\n",
      "2022-03-01 08:35:57,010 —  INFO — Image flattening complete!\n"
     ]
    }
   ],
   "source": [
    "# Image flattening for training data\n",
    "logger.info(f'Flattening images...')\n",
    "flat_arr = []\n",
    "\n",
    "for group in img_arr:\n",
    "    flat_arr.append(list())\n",
    "    for img in group:\n",
    "        flat_img = img.flatten()\n",
    "        flat_arr[len(flat_arr)-1].append(flat_img)        \n",
    "        \n",
    "logger.info(f'Image flattening complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-01 08:36:01,821 —  INFO — ======= Start Of Data Split =======\n",
      "2022-03-01 08:36:04,095 —  INFO — Splitted Successfully. Training Set Length = 146\n",
      "2022-03-01 08:36:04,096 —  INFO — Test Splitted Successfully\n",
      "2022-03-01 08:36:04,097 —  INFO — ======= End Of Data Split =======\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Slicing of input data into seperate dataframes (two for each data set), one containing image data and the other containing class data\n",
    "\n",
    "list_arr_fun = lambda l : np.array(l)\n",
    "flat_img_list=list(map(list_arr_fun, flat_arr))\n",
    "label_img_list=list(map(list_arr_fun, label_arr))\n",
    "\n",
    "df_Training=pd.DataFrame(np.append(flat_img_list[0], flat_img_list[1], axis=0))\n",
    "\n",
    "df_Training['label']=np.append(label_img_list[0], label_img_list[1], axis=0)\n",
    "\n",
    "x_Training=df_Training.iloc[:,:-1] # Image data \n",
    "y_Training=df_Training.iloc[:,-1] # Label data\n",
    "\n",
    "# Creates test and training split\n",
    "logger.info('======= Start Of Data Split =======')\n",
    "x_train,x_test,y_train,y_test=train_test_split(x_Training,y_Training, test_size=0.20,random_state=77,stratify=y_Training)\n",
    "# We want all the images included in training set, thus the training and test sets are concatinated\n",
    "x_train = pd.concat([x_train, x_test]) \n",
    "y_train = pd.concat([y_train, y_test])\n",
    "logger.info(f'Splitted Successfully. Training Set Length = {len(y_train)}')\n",
    "\n",
    "logger.info('Test Splitted Successfully')\n",
    "logger.info('======= End Of Data Split =======\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A generalized model that trains a given model, gathering time complexity stats about its training.\n",
    "\n",
    "def modelTrainer(model, x_train, y_train, model_name, method_name):\n",
    "\n",
    "    pipe = make_pipeline(model) # StandardScaler() was removed as it appears to negatively affect the baseline\n",
    "    logger.info(f\"Starting to train {method_name} model...\")\n",
    "    start = time.time()\n",
    "    pipe.fit(x_train,y_train)\n",
    "    end = time.time()\n",
    "    logger.info(f\"The {model_name} model trained in: {str(end - start)} seconds\")\n",
    "    return pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-01 08:36:04,296 —  INFO — ======= Start Of Logistic Regression Model Generation =======\n",
      "2022-03-01 08:36:04,297 —  INFO — Starting to train L1_LogR model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-01 08:45:21,707 —  INFO — The L1_LogR model trained in: 557.4090864658356 seconds\n",
      "2022-03-01 08:45:21,724 —  INFO — Successfully created trained models pickle\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Search for Pickle of previously trained models.\n",
    "Pickle_Path = glob.glob(os.path.join(Pickle_location, Models_Pickle_Name))\n",
    "if len(Pickle_Path) == 0:\n",
    "    # Train all models\n",
    "    \n",
    "    logger.info('======= Start Of Logistic Regression Model Generation =======')\n",
    "    LogR_param_grid= {'penalty':['l1'], 'C':[0.1,1,10,100], 'max_iter':[2048], 'solver': ['liblinear', 'saga',] } \n",
    "    LogR_model= GridSearchCV(LogisticRegression(), LogR_param_grid, n_jobs=-1)\n",
    "    LogR_model = modelTrainer(LogR_model, x_train, y_train, \"L1_LogR\", \"L1_LogR\")\n",
    "\n",
    "    # logger.info('======= Start Of SVM Model Generation =======')\n",
    "    # SVM_param_grid={'C':[0.1,1,10,100],'gamma':[0.0001,0.001,0.1,1],'kernel':['rbf','poly']}\n",
    "    # svc=svm.SVC(probability=True)\n",
    "    # SVM_model= GridSearchCV(svm.SVC(probability=True), SVM_param_grid, n_jobs=-1)\n",
    "    # SVM_model= modelTrainer(SVM_model, x_train, y_train, \"SVM\", \"SVM\")\n",
    "\n",
    "    # logger.info('======= Start Of K Nearest Neighbors Model Generation =======')\n",
    "    # KNN_param_grid={'n_neighbors':[5], 'weights':['uniform', 'distance']}\n",
    "    # KNN_model= GridSearchCV(KNeighborsClassifier(), KNN_param_grid, n_jobs=-1)\n",
    "    # KNN_model= modelTrainer(KNN_model, x_train, y_train, \"KNN\", \"KNN\")\n",
    "\n",
    "    # logger.info('======= Start Of Random Forest Model Generation =======')\n",
    "    # RanF_param_grid={'n_estimators':[100], 'criterion': ['gini', 'entropy']}\n",
    "    # RanF_model= GridSearchCV(RandomForestClassifier(), RanF_param_grid, n_jobs=-1)\n",
    "    # RanF_model= modelTrainer(RanF_model, x_train, y_train, \"RanF\", \"RanF\")\n",
    "\n",
    "    Trained_Models = [LogR_model]\n",
    "    with open(os.path.join(Pickle_location, Models_Pickle_Name),\"wb\") as file_handle:\n",
    "        pickle.dump(Trained_Models, file_handle, pickle.HIGHEST_PROTOCOL)\n",
    "    logger.info(f'Successfully created trained models pickle')\n",
    "else:\n",
    "    logger.info('Found Pickle, starting unpickling...')\n",
    "    with open(os.path.join(Pickle_location, Eval_Data_Pickle_Name),\"rb\") as file_handle:\n",
    "        Trained_Models = pickle.load(file_handle)\n",
    "        logger.info(f'Successfully loaded models pickle!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying the location of data to be evaluated by the models.\n",
    "Eval_Inpath = \"/birl2/users/sch923/Thesis/Data/phenoSEEDOutput\"\n",
    "Eval_Labels = \"/birl2/users/sch923/Thesis/Data/UGRep2FDK.txt\"\n",
    "Eval_Extension = '*.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n",
      "/birl2/users/sch923/.conda/envs/thesis_env/lib/python3.7/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-01 08:46:15,047 —  INFO — Successfully loaded eval images pickle!\n",
      "2022-03-01 08:46:15,048 —  INFO — Successfully loaded evaluation images, found 46586 images\n"
     ]
    }
   ],
   "source": [
    "# Search for Pickle of previously loaded imaged. Saves time vs re-indexing and pre-processing images.\n",
    "Pickle_Path = glob.glob(os.path.join(Pickle_location, Eval_Data_Pickle_Name))\n",
    "\n",
    "if len(Pickle_Path) == 0:\n",
    "    # Collect all images (in the form of .npz) to be evaluated by the networks\n",
    "    Evale_Images_Paths = glob.glob(os.path.join(Eval_Inpath, '**', Eval_Extension), recursive=True)\n",
    "    if len(Evale_Images_Paths) == 0:\n",
    "        logging.error(f' there are no {Eval_Extension} found in supplied directory: \\n{Eval_Inpath}')\n",
    "\n",
    "\n",
    "    # Preform an identical pre-procesisng step as images used for training.\n",
    "    Eval_img_tuple_arr= [] # contains lot name, resized image\n",
    "    for img_path in Evale_Images_Paths:\n",
    "        img=imread(os.path.join(path,img_path))\n",
    "        Eval_img_resized = resize(img,(150,150,3))\n",
    "        Eval_flat_img = Eval_img_resized.flatten()\n",
    "        Eval_flat_img_arr = np.array(Eval_flat_img)\n",
    "        Eval_flat_df=pd.DataFrame(Eval_flat_img_arr)\n",
    "\n",
    "        lot_name = img_path.split('/')[-3] # Based on the file path we can derive the lot name  \n",
    "        Eval_img_tuple_arr.append((lot_name, Eval_flat_df)) # Saves flat images\n",
    "    \n",
    "    with open(os.path.join(Pickle_location, Eval_Data_Pickle_Name),\"wb\") as file_handle:\n",
    "        pickle.dump(Eval_img_tuple_arr, file_handle, pickle.HIGHEST_PROTOCOL)\n",
    "    logger.info(f'Successfully created evaluation image pickle')\n",
    "else:\n",
    "    with open(os.path.join(Pickle_location, Eval_Data_Pickle_Name),\"rb\") as file_handle:\n",
    "        Eval_img_tuple_arr = pickle.load(file_handle)\n",
    "        logger.info(f'Successfully loaded eval images pickle!')\n",
    "\n",
    "logger.info(f'Successfully loaded evaluation images, found {len(Eval_img_tuple_arr)} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-01 08:46:15,202 —  INFO — Starting predictions for L1_LogR model...\n",
      "2022-03-01 13:28:18,772 —  INFO — Successfully created predictions pickle\n"
     ]
    }
   ],
   "source": [
    "Model_Label = ['L1_LogR']\n",
    "# Model_Label = ['Logistic Regression', 'Support Vector Machine', 'K Nearest Neighbors', 'Random Forest'] # NOTE, these label names need to be in the same order as the models in Trained_Models\n",
    "Prediction_dict_arr = []\n",
    "\n",
    "Pickle_Path = glob.glob(os.path.join(Pickle_location, Prediction_Pickle_Name))\n",
    "\n",
    "if len(Pickle_Path) == 0:\n",
    "    for model_name, model in zip(Model_Label, Trained_Models):\n",
    "        Infected_count = 0\n",
    "        Kernel_count = 0\n",
    "        Infected_Percent_dict = dict()\n",
    "        Last_label = \"\"\n",
    "        logger.info(f'Starting predictions for {model_name} model...')\n",
    "        for (label, img) in Eval_img_tuple_arr:\n",
    "            if not label in Infected_Percent_dict.keys():\n",
    "                # Guard for first loop iteration\n",
    "                if not Last_label == \"\":\n",
    "                    Infected_Percent_dict[Last_label]= (Infected_count/Kernel_count, Infected_count, Kernel_count) # Save relevant counts for last label\n",
    "                    Infected_count = 0\n",
    "                    Kernel_count = 0\n",
    "                    Last_label = label\n",
    "                else: \n",
    "                    Last_label = label\n",
    "            \n",
    "            pred = model.predict(img.T)\n",
    "            \n",
    "            if pred[0] == 1:\n",
    "                Infected_count = Infected_count + 1  \n",
    "            Kernel_count = Kernel_count + 1\n",
    "            \n",
    "        # Save each models prediction results in list\n",
    "        Prediction_dict_arr.append((model_name, Infected_Percent_dict))\n",
    "\n",
    "    with open(os.path.join(Pickle_location, Prediction_Pickle_Name),\"wb\") as file_handle:\n",
    "        pickle.dump(Prediction_dict_arr, file_handle, pickle.HIGHEST_PROTOCOL)\n",
    "    logger.info(f'Successfully created predictions pickle')\n",
    "\n",
    "else:\n",
    "    with open(os.path.join(Pickle_location, Prediction_Pickle_Name),\"rb\") as file_handle:\n",
    "        Prediction_dict_arr = pickle.load(file_handle)\n",
    "        logger.info(f'Successfully loaded prediction pickle!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-01 13:28:19,098 —  INFO — Loaded labels from /birl2/users/sch923/Thesis/Data/UGRep2FDK.txt\n",
      "2022-03-01 13:28:19,098 —  INFO — Starting metric gathering for L1_LogR...\n",
      "2022-03-01 13:28:19,104 —  INFO — Completed metric gathering for L1_LogR!\n",
      "2022-03-01 13:28:19,129 —  INFO — Saved gathered metrics to csv!\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# Get all image metadata\n",
    "# CN, Abbrev., Greenhouse Entry, Tall/Short, U of G FDK, U of G FDK%, UofG Imaging #\n",
    "Eval_Label_List = pd.read_csv(Eval_Labels, delimiter='\\t').values.tolist()\n",
    "logger.info(f'Loaded labels from {Eval_Labels}')\n",
    "headers = ['ModelName', 'Abbrev.', 'PredictedInfected', 'ActualInfected', 'Accuracy']\n",
    "metrics_rows = []\n",
    "for (model_name, Prediction_dict) in Prediction_dict_arr:\n",
    "    logger.info(f'Starting metric gathering for {model_name}...')\n",
    "    for Eval_Entry in Eval_Label_List:\n",
    "        if Eval_Entry[1] in Prediction_dict and not math.isnan(Eval_Entry[4]) and Eval_Entry[4] > 0.0 and float(Prediction_dict[Eval_Entry[1]][1]) > 0.0:\n",
    "            metrics = []\n",
    "            metrics.append(model_name)\n",
    "            metrics.append(Eval_Entry[1])\n",
    "            metrics.append(Eval_Entry[4])  # The index of the U of G FDK or the number of infected kernels\n",
    "            metrics.append(float(Prediction_dict[Eval_Entry[1]][1])) # The index of the Abbrev. lot label, the index of the # infected predicted\n",
    "            if Eval_Entry[4] > float(Prediction_dict[Eval_Entry[1]][1]):\n",
    "                metrics.append(float(Prediction_dict[Eval_Entry[1]][1]/Eval_Entry[4]))\n",
    "            else:\n",
    "                metrics.append(Eval_Entry[4]/float(Prediction_dict[Eval_Entry[1]][1]))\n",
    "            metrics_rows.append(metrics\n",
    "    # Generater confusion matrix\n",
    "    )\n",
    "    logger.info(f'Completed metric gathering for {model_name}!')\n",
    "\n",
    "pd.DataFrame(metrics_rows, columns =headers).to_csv('L1_LogR_Prediction.csv',index=False)\n",
    "# pd.DataFrame(metrics_rows, columns =headers).to_csv('Full_Set_Prediction.csv',index=False)\n",
    "\n",
    "\n",
    "logger.info(f'Saved gathered metrics to csv!')   \n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1ba9ac630c9f2dd3511fdfeda2d56f08e3335139f33aea9e4eda54962c7821d0"
  },
  "kernelspec": {
   "display_name": "Python (thesis_env)",
   "language": "python",
   "name": "thesis_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
